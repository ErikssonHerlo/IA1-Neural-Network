{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Implementation\n",
    "\n",
    "This project implements a neural network capable of learning logical operations such as AND and OR. The neural network is configurable in terms of input size, output size, hidden layers, and activation functions.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Activation Functions](#activation-functions)\n",
    "   - [Sigmoid](#sigmoid)\n",
    "   - [Tanh](#tanh)\n",
    "   - [ReLU](#relu)\n",
    "   - [Leaky ReLU](#leaky-relu)\n",
    "   - [Softmax](#softmax)\n",
    "   - [Step](#step)\n",
    "3. [Recommended Combinations](#recommended-combinations)\n",
    "   - [Hidden Layers](#hidden-layers)\n",
    "   - [Output Layers](#output-layers)\n",
    "4. [Example Configuration](#example-configuration)\n",
    "5. [Usage](#usage)\n",
    "6. [Implementation Details](#implementation-details)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project allows you to create, train, and test a neural network for simple logical operations. You can configure the number of inputs, outputs, hidden layers, and choose from several activation functions.\n",
    "\n",
    "## Activation Functions\n",
    "\n",
    "### Sigmoid\n",
    "\n",
    "- **Function:** \n",
    "  \\[\n",
    "  \\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "  \\]\n",
    "- **Derivative:**\n",
    "  \\[\n",
    "  \\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "  \\]\n",
    "- **Characteristics:**\n",
    "  - Maps any input value to a range between 0 and 1.\n",
    "  - Commonly used in the output layer for binary classification problems.\n",
    "  - Issues: Can suffer from vanishing gradient and saturation.\n",
    "- **Recommended Use:** Output layer for binary classification problems.\n",
    "\n",
    "### Tanh\n",
    "\n",
    "- **Function:**\n",
    "  \\[\n",
    "  \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "  \\]\n",
    "- **Derivative:**\n",
    "  \\[\n",
    "  \\tanh'(x) = 1 - \\tanh(x)^2\n",
    "  \\]\n",
    "- **Characteristics:**\n",
    "  - Maps input values to a range between -1 and 1.\n",
    "  - Preferred over sigmoid for hidden layers because its output is zero-centered.\n",
    "  - Issues: Can also suffer from vanishing gradient.\n",
    "- **Recommended Use:** Hidden layers.\n",
    "\n",
    "### ReLU (Rectified Linear Unit)\n",
    "\n",
    "- **Function:**\n",
    "  \\[\n",
    "  \\text{ReLU}(x) = \\max(0, x)\n",
    "  \\]\n",
    "- **Derivative:**\n",
    "  \\[\n",
    "  \\text{ReLU}'(x) = \\begin{cases} \n",
    "  1 & \\text{if } x > 0 \\\\\n",
    "  0 & \\text{if } x \\leq 0\n",
    "  \\end{cases}\n",
    "  \\]\n",
    "- **Characteristics:**\n",
    "  - Maps any input value to 0 or the value itself if it is positive.\n",
    "  - Very popular for hidden layers due to its simplicity and efficiency.\n",
    "  - Issues: Can suffer from \"dead neuron\" problem when many units are inactive.\n",
    "- **Recommended Use:** Hidden layers.\n",
    "\n",
    "### Leaky ReLU\n",
    "\n",
    "- **Function:**\n",
    "  \\[\n",
    "  \\text{Leaky ReLU}(x) = \\begin{cases} \n",
    "  x & \\text{if } x > 0 \\\\\n",
    "  \\alpha x & \\text{if } x \\leq 0\n",
    "  \\end{cases}\n",
    "  \\]\n",
    "- **Derivative:**\n",
    "  \\[\n",
    "  \\text{Leaky ReLU}'(x) = \\begin{cases} \n",
    "  1 & \\text{if } x > 0 \\\\\n",
    "  \\alpha & \\text{if } x \\leq 0\n",
    "  \\end{cases}\n",
    "  \\]\n",
    "- **Characteristics:**\n",
    "  - Similar to ReLU but allows a small gradient when the input is negative.\n",
    "- **Recommended Use:** Hidden layers.\n",
    "\n",
    "### Softmax\n",
    "\n",
    "- **Function:**\n",
    "  \\[\n",
    "  \\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
    "  \\]\n",
    "- **Characteristics:**\n",
    "  - Used for multi-class classification problems.\n",
    "  - Converts input values to probabilities that sum to 1.\n",
    "- **Recommended Use:** Output layer for multi-class classification problems.\n",
    "\n",
    "### Step (Binary Step)\n",
    "\n",
    "- **Function:**\n",
    "  \\[\n",
    "  \\text{Step}(x) = \\begin{cases} \n",
    "  1 & \\text{if } x \\geq 0 \\\\\n",
    "  0 & \\text{if } x < 0\n",
    "  \\end{cases}\n",
    "  \\]\n",
    "- **Characteristics:**\n",
    "  - Produces binary output.\n",
    "  - Not commonly used in modern neural networks due to its zero gradient everywhere except at zero.\n",
    "- **Recommended Use:** Not common in modern neural networks.\n",
    "\n",
    "## Recommended Combinations\n",
    "\n",
    "### Hidden Layers\n",
    "\n",
    "1. **ReLU/Leaky ReLU:** Most common due to efficiency and good performance in practice.\n",
    "2. **Tanh:** Good for older networks or when outputs need to be zero-centered.\n",
    "3. **Sigmoid:** Less common for hidden layers due to vanishing gradient issues.\n",
    "\n",
    "### Output Layers\n",
    "\n",
    "1. **Sigmoid:** For binary classification problems.\n",
    "2. **Softmax:** For multi-class classification problems.\n",
    "3. **Identity:** For regression problems.\n",
    "4. **ReLU/Leaky ReLU:** Occasionally used for non-negative regression outputs.\n",
    "\n",
    "## Example Configuration\n",
    "\n",
    "For a binary classification problem (e.g., AND operation):\n",
    "\n",
    "- **Inputs:** 2\n",
    "- **Outputs:** 1\n",
    "- **Hidden Layers:** 2 layers with 2 neurons each\n",
    "- **Activation Function for Hidden Layers:** `sigmoid` (or `ReLU`)\n",
    "- **Activation Function for Output Layer:** `sigmoid`\n",
    "- **Epochs:** 1000\n",
    "- **Learning Rate:** 0.1\n",
    "\n",
    "Example Input:\n",
    "```plaintext\n",
    "Bienvenido al configurador de Redes Neuronales!\n",
    "Ingrese el nÃºmero de entradas: 2\n",
    "Ingrese el nÃºmero de salidas: 1\n",
    "Ingrese el nÃºmero de neuronas en cada capa oculta, separadas por comas: 2,2\n",
    "Ingrese la funciÃ³n de activaciÃ³n para las capas ocultas (sigmoid/tanh): sigmoid\n",
    "Ingrese la funciÃ³n de activaciÃ³n para la capa de salida (sigmoid/identity): sigmoid\n",
    "Ingrese el nÃºmero de Ã©pocas para el entrenamiento: 1000\n",
    "Ingrese la tasa de aprendizaje: 0.1\n",
    "Ingrese la matriz de caracterÃ­sticas de entrenamiento (X) en formato de lista de listas: [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "Ingrese la matriz de resultados esperados (y) en formato de lista de listas: [[0], [0], [0], [1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU(x)=max(0,x)\n",
    "Derivada:\n",
    "\n",
    "ReLU\n",
    "â€²\n",
    "(\n",
    "ð‘¥\n",
    ")\n",
    "=\n",
    "{\n",
    "1\n",
    "si \n",
    "ð‘¥\n",
    ">\n",
    "0\n",
    "0\n",
    "si \n",
    "ð‘¥\n",
    "â‰¤\n",
    "0\n",
    "ReLU \n",
    "â€²\n",
    " (x)={ \n",
    "1\n",
    "0\n",
    "â€‹\n",
    "  \n",
    "si x>0\n",
    "si xâ‰¤0\n",
    "â€‹\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
