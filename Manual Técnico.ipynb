{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación de Red Neuronal\n",
    "\n",
    "Este proyecto implementa una red neuronal capaz de aprender operaciones lógicas como AND, OR y XOR. La red neuronal es configurable en términos de tamaño de entrada, tamaño de salida, capas ocultas y funciones de activación.\n",
    "\n",
    "## Tabla de Contenidos\n",
    "\n",
    "1. [Introducción](#introducción)\n",
    "2. [Funciones de Activación](#funciones-de-activación)\n",
    "   - [Sigmoide](#sigmoide)\n",
    "   - [Tanh](#tanh)\n",
    "   - [ReLU](#relu)\n",
    "   - [Leaky ReLU](#leaky-relu)\n",
    "   - [Step](#step)\n",
    "3. [Clase Red Neuronal](#clase-red-neuronal)\n",
    "   - [Inicialización](#inicialización)\n",
    "   - [Propagación Hacia Adelante](#propagación-hacia-adelante)\n",
    "   - [Propagación Hacia Atrás](#propagación-hacia-atrás)\n",
    "   - [Entrenamiento](#entrenamiento)\n",
    "   - [Predicción](#predicción)\n",
    "4. [Combinaciones Recomendadas](#combinaciones-recomendadas)\n",
    "   - [Capas Ocultas](#capas-ocultas)\n",
    "   - [Capas de Salida](#capas-de-salida)\n",
    "5. [Configuración de Ejemplo](#configuración-de-ejemplo)\n",
    "6. [Uso](#uso)\n",
    "\n",
    "## Introducción\n",
    "\n",
    "Este proyecto permite crear, entrenar y probar una red neuronal para operaciones lógicas simples. Puedes configurar el número de entradas, salidas, capas ocultas y elegir entre varias funciones de activación.\n",
    "\n",
    "## Funciones de Activación\n",
    "\n",
    "### Sigmoide\n",
    "\n",
    "- **Función:** \n",
    "  \\[\n",
    "  \\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "  \\]\n",
    "- **Derivada:**\n",
    "  \\[\n",
    "  \\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "  \\]\n",
    "- **Características:**\n",
    "  - Mapea cualquier valor de entrada a un rango entre 0 y 1.\n",
    "  - Comúnmente utilizada en la capa de salida para problemas de clasificación binaria.\n",
    "  - Problemas: Puede sufrir de desvanecimiento de gradiente y saturación.\n",
    "- **Uso Recomendado:** Capa de salida para problemas de clasificación binaria.\n",
    "\n",
    "### Tanh\n",
    "\n",
    "- **Función:**\n",
    "  \\[\n",
    "  \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "  \\]\n",
    "- **Derivada:**\n",
    "  \\[\n",
    "  \\tanh'(x) = 1 - \\tanh(x)^2\n",
    "  \\]\n",
    "- **Características:**\n",
    "  - Mapea los valores de entrada a un rango entre -1 y 1.\n",
    "  - Preferida sobre la sigmoide para capas ocultas porque su salida está centrada en cero.\n",
    "  - Problemas: También puede sufrir de desvanecimiento de gradiente.\n",
    "- **Uso Recomendado:** Capas ocultas.\n",
    "\n",
    "### ReLU (Rectified Linear Unit)\n",
    "\n",
    "- **Función:**\n",
    "  \\[\n",
    "  \\text{ReLU}(x) = \\max(0, x)\n",
    "  \\]\n",
    "- **Derivada:**\n",
    "  \\[\n",
    "  \\text{ReLU}'(x) = \\begin{cases} \n",
    "  1 & \\text{si } x > 0 \\\\\n",
    "  0 & \\text{si } x \\leq 0\n",
    "  \\end{cases}\n",
    "  \\]\n",
    "- **Características:**\n",
    "  - Mapea cualquier valor de entrada a 0 o el valor mismo si es positivo.\n",
    "  - Muy popular para capas ocultas debido a su simplicidad y eficiencia.\n",
    "  - Problemas: Puede sufrir del problema de \"neurona muerta\" cuando muchas unidades están inactivas.\n",
    "- **Uso Recomendado:** Capas ocultas.\n",
    "\n",
    "### Leaky ReLU\n",
    "\n",
    "- **Función:**\n",
    "  \\[\n",
    "  \\text{Leaky ReLU}(x) = \\begin{cases} \n",
    "  x & \\text{si } x > 0 \\\\\n",
    "  \\alpha x & \\text{si } x \\leq 0\n",
    "  \\end{cases}\n",
    "  \\]\n",
    "- **Derivada:**\n",
    "  \\[\n",
    "  \\text{Leaky ReLU}'(x) = \\begin{cases} \n",
    "  1 & \\text{si } x > 0 \\\\\n",
    "  \\alpha & \\text{si } x \\leq 0\n",
    "  \\end{cases}\n",
    "  \\]\n",
    "- **Características:**\n",
    "  - Similar a ReLU pero permite un pequeño gradiente cuando la entrada es negativa.\n",
    "- **Uso Recomendado:** Capas ocultas.\n",
    "\n",
    "### Step (Función Escalón)\n",
    "\n",
    "- **Función:**\n",
    "  \\[\n",
    "  \\text{Step}(x) = \\begin{cases} \n",
    "  1 & \\text{si } x \\geq 0 \\\\\n",
    "  0 & \\text{si } x < 0\n",
    "  \\end{cases}\n",
    "  \\]\n",
    "- **Características:**\n",
    "  - Produce una salida binaria.\n",
    "  - No utilizada comúnmente en redes neuronales modernas debido a su derivada cero en todas partes excepto en cero.\n",
    "- **Uso Recomendado:** No es común en redes neuronales modernas.\n",
    "\n",
    "## Clase Red Neuronal\n",
    "\n",
    "### Inicialización\n",
    "\n",
    "La clase `NeuralNetwork` se inicializa con los siguientes parámetros:\n",
    "- `input_size`: Número de neuronas de entrada.\n",
    "- `output_size`: Número de neuronas de salida.\n",
    "- `hidden_layers`: Lista que contiene el número de neuronas en cada capa oculta.\n",
    "- `hidden_activation`: Función de activación para las capas ocultas (por defecto es `relu`).\n",
    "- `output_activation`: Función de activación para la capa de salida (por defecto es `sigmoid` durante el entrenamiento, pero puede cambiarse a `step` para la predicción final).\n",
    "\n",
    "```python\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, output_size, hidden_layers, hidden_activation='relu', output_activation='sigmoid'):\n",
    "```\n",
    "\n",
    "## Inicialización de Pesos y Biases:\n",
    "\n",
    "- Los pesos y biases se inicializan para cada capa de la red.\n",
    "- Los pesos se inicializan con pequeños valores aleatorios, y los biases se inicializan en cero.\n",
    "\n",
    "```python\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        layer_sizes = [input_size] + hidden_layers + [output_size]\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.1)\n",
    "            self.biases.append(np.zeros(layer_sizes[i+1]))\n",
    "```\n",
    "\n",
    "2. **Funciones de Activación:**\n",
    "   - Se seleccionan las funciones de activación apropiadas y sus derivadas según los parámetros proporcionados.\n",
    "\n",
    "```python\n",
    "        self.hidden_activation = self.get_activation_function(hidden_activation)\n",
    "        self.hidden_activation_derivative = self.get_activation_derivative(hidden_activation)\n",
    "        self.output_activation = self.get_activation_function(output_activation)\n",
    "        self.output_activation_derivative = self.get_activation_derivative(output_activation)\n",
    "        \n",
    "```\n",
    "\n",
    "### Propagación Hacia Adelante\n",
    "\n",
    "El método `forward` calcula la salida de la red neuronal para una entrada dada `X`. Esto implica pasar la entrada a través de cada capa de la red y aplicar las funciones de activación.\n",
    "\n",
    "```python\n",
    "    def forward(self, X):\n",
    "        self.activations = [X]\n",
    "        input_to_layer = X\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            output_from_layer = self.hidden_activation(np.dot(input_to_layer, self.weights[i]) + self.biases[i])\n",
    "            self.activations.append(output_from_layer)\n",
    "            input_to_layer = output_from_layer\n",
    "        final_output = self.output_activation(np.dot(input_to_layer, self.weights[-1]) + self.biases[-1])\n",
    "        self.activations.append(final_output)\n",
    "        return final_output\n",
    "```\n",
    "\n",
    "### Propagación Hacia Atrás\n",
    "\n",
    "El método `backpropagate` ajusta los pesos y biases basados en el error entre la salida predicha y la salida real. Este método realiza los siguientes pasos:\n",
    "\n",
    "1. **Calcular el Error:**\n",
    "   - El error se calcula como la diferencia entre la salida real `y` y la salida predicha `output`.\n",
    "\n",
    "```python\n",
    "        error = y - output\n",
    "```\n",
    "\n",
    "2. **Calcular las Deltas para la Capa de Salida:**\n",
    "   - La delta para la capa de salida se calcula multiplicando el error por la derivada de la función de activación de la capa de salida.\n",
    "\n",
    "```python\n",
    "        deltas = [error * self.output_activation_derivative(output)]\n",
    "```\n",
    "\n",
    "3. **Calcular las Deltas para las Capas Ocultas:**\n",
    "   - Las deltas para las capas ocultas se calculan propagando el error hacia atrás a través de la red, capa por capa.\n",
    "\n",
    "```python\n",
    "        for i in range(len(self.weights) - 2, -1, -1):\n",
    "            deltas.append(deltas[-1].dot(self.weights[i + 1].T) * self.hidden_activation_derivative(self.activations[i + 1]))\n",
    "        deltas.reverse()\n",
    "```\n",
    "\n",
    "4. **Actualizar Pesos y Biases:**\n",
    "   - Los pesos y biases se actualizan utilizando las deltas calculadas y la tasa de aprendizaje.\n",
    "\n",
    "```python\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] += learning_rate * self.activations[i].T.dot(deltas[i])\n",
    "            self.biases[i] += learning_rate * np.sum(deltas[i], axis=0)\n",
    "```\n",
    "\n",
    "### Entrenamiento\n",
    "\n",
    "El método `train` entrena la red neuronal durante un número específico de épocas utilizando los datos de entrenamiento proporcionados y la tasa de aprendizaje.\n",
    "\n",
    "```python\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            self.backpropagate(X, y, learning_rate)\n",
    "            if epoch % 100 == 0:\n",
    "                loss = np.mean(np.square(y - self.forward(X)))\n",
    "                predictions = self.forward(X)\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')\n",
    "                print(\"Predicciones (entrenamiento):\")\n",
    "                for i in range(len(X)):\n",
    "                    print(f\"Entrada: {X[i]}, Predicción: {predictions[i]}\")\n",
    "                print(\"\\nPesos y biases:\")\n",
    "                for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n",
    "                    print(f\"Capa {i} - Pesos:\\n{w}\\nBiases:\\n{b}\")\n",
    "```\n",
    "\n",
    "### Predicción\n",
    "\n",
    "El método `predict` utiliza la red neuronal entrenada para hacer predicciones en nuevos datos. La función `step` se aplica a la salida para convertirla en valores binarios.\n",
    "\n",
    "```python\n",
    "    def predict(self, X):\n",
    "        raw_output = self.forward(X)\n",
    "        return raw_output\n",
    "```\n",
    "\n",
    "## Combinaciones Recomendadas\n",
    "\n",
    "### Capas Ocultas\n",
    "\n",
    "1. **ReLU/Leaky ReLU:** Más comunes debido a su eficiencia y buen rendimiento en la práctica.\n",
    "2. **Tanh:** Buenas para redes más antiguas o cuando las salidas necesitan estar centradas en cero.\n",
    "3. **Sigmoide:** Menos comunes para capas ocultas debido a problemas de desvanecimiento de gradiente.\n",
    "\n",
    "### Capas de Salida\n",
    "\n",
    "1. **Sigmoide:** Para problemas de clasificación binaria.\n",
    "2. **Identity:** Para problemas de regresión.\n",
    "3. **Step:** Se aplica a la salida para convertirla en valores binarios, ya que solo devuelve valores 0 y 1.\n",
    "\n",
    "## Configuración de Ejemplo\n",
    "\n",
    "Para un problema de clasificación binaria (e.g., operación AND):\n",
    "\n",
    "- **Entradas:** 2\n",
    "- **Salidas:** 1\n",
    "- **Capas Ocultas:** 2 capas con 2 neuronas cada una\n",
    "- **Función de Activación para Capas Ocultas:** `sigmoid` (o `ReLU`)\n",
    "- **Función de Activación para la Capa de Salida:** `sigmoid`\n",
    "- **Épocas:** 1000\n",
    "- **Tasa de Aprendizaje:** 0.1\n",
    "\n",
    "Entrada de Ejemplo:\n",
    "```plaintext\n",
    "Bienvenido al configurador de Redes Neuronales!\n",
    "Ingrese el número de entradas: 2\n",
    "Ingrese el número de salidas: 1\n",
    "Ingrese el número de neuronas en cada capa oculta, separadas por comas: 2,2\n",
    "Ingrese la función de activación para las capas ocultas (sigmoid/tanh): sigmoid\n",
    "Ingrese la función de activación para la capa de salida (sigmoid/identity): sigmoid\n",
    "Ingrese el número de épocas para el entrenamiento: 1000\n",
    "Ingrese la tasa de aprendizaje: 0.1\n",
    "Ingrese la matriz de características de entrenamiento (X) en formato de lista de listas: [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "Ingrese la matriz de resultados esperados (y) en formato de lista de listas: [[0], [0], [0], [1]]\n",
    "```\n",
    "\n",
    "## Uso\n",
    "\n",
    "```python\n",
    "from NeuralNetwork import NeuralNetwork\n",
    "import numpy as np\n",
    "\n",
    "def main():\n",
    "    print(\"Bienvenido al configurador de Redes Neuronales!\")\n",
    "    input_size = int(input(\"Ingrese el número de entradas: \"))\n",
    "    output_size = int(input(\"Ingrese el número de salidas: \"))\n",
    "    hidden_layers = list(map(int, input(\"Ingrese el número de neuronas en cada capa oculta, separadas por comas: \").split(',')))\n",
    "    hidden_activation = input(\"Ingrese la función de activación para las capas ocultas (sigmoid/tanh): \").lower() or 'sigmoid'\n",
    "    output_activation = input(\"Ingrese la función de activación para la capa de salida (sigmoid/identity/step): \").lower() or 'sigmoid'\n",
    "    epochs = int(input(\"Ingrese el número de épocas para el entrenamiento: 1000\"))\n",
    "    learning_rate = float(input(\"Ingrese la tasa de aprendizaje: 0.1\"))\n",
    "\n",
    "    X = np.array(eval(input(\"Ingrese la matriz de características de entrenamiento (X) en formato de lista de listas: \")))\n",
    "    y = np.array(eval(input(\"Ingrese la matriz de resultados esperados (y) en formato de lista de listas: \")))\n",
    "\n",
    "    nn = NeuralNetwork(input_size, output_size, hidden_layers, hidden_activation, output_activation)\n",
    "    nn.train(X, y, epochs, learning_rate)\n",
    "\n",
    "    print(\"Entrenamiento completado. Realizando predicciones con los mismos datos de entrenamiento.\")\n",
    "    predictions = nn.predict(X)\n",
    "\n",
    "    print(\"\\nEntradas y Predicciones:\")\n",
    "    for i in range(len(X)):\n",
    "        print(f\"Entrada: {X[i]}, Predicción: {predictions[i]}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "Este README proporciona una explicación detallada del funcionamiento de cada parte de la clase `NeuralNetwork`, incluyendo la inicialización, propagación hacia adelante, propagación hacia atrás, entrenamiento y predicción. Además, se ofrecen recomendaciones para las combinaciones de funciones de activación en capas ocultas y de salida, junto con un ejemplo de configuración y uso del código.\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
